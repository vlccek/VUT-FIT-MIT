## THIS WAS GENERATED BY CHATGPT. Yes im serious.

import unittest
import numpy as np
from sui_torch import Tensor, add, subtract, multiply, dot_product, sui_sum, relu

# Assuming the neural network library code is in a file named 'nn_library.py'
# from nn_library import Tensor, add, subtract, multiply, dot_product, sui_sum, relu

# For the purpose of this test, I will redefine the required functions inline.
# In your actual test file, you should import these from your library as shown above.

class TestNNLibrary(unittest.TestCase):
    def test_add_forward(self):
        a = Tensor(np.array([1.0, 2.0]))
        b = Tensor(np.array([3.0, 4.0]))
        c = add(a, b)
        expected = np.array([4.0, 6.0])
        np.testing.assert_array_equal(c.value, expected, "Addition forward pass failed.")

    def test_add_backward(self):
        a = Tensor(np.array([1.0, 2.0]))
        b = Tensor(np.array([3.0, 4.0]))
        c = add(a, b)
        c.backward(np.array([1.0, 1.0]))
        expected_grad_a = np.array([1.0, 1.0])
        expected_grad_b = np.array([1.0, 1.0])
        np.testing.assert_array_equal(a.grad, expected_grad_a, "Addition backward pass failed for a.")
        np.testing.assert_array_equal(b.grad, expected_grad_b, "Addition backward pass failed for b.")

    def test_subtract_forward(self):
        a = Tensor(np.array([5.0, 6.0]))
        b = Tensor(np.array([3.0, 2.0]))
        c = subtract(a, b)
        expected = np.array([2.0, 4.0])
        np.testing.assert_array_equal(c.value, expected, "Subtraction forward pass failed.")

    def test_subtract_backward(self):
        a = Tensor(np.array([5.0, 6.0]))
        b = Tensor(np.array([3.0, 2.0]))
        c = subtract(a, b)
        c.backward(np.array([1.0, 1.0]))
        expected_grad_a = np.array([1.0, 1.0])
        expected_grad_b = np.array([-1.0, -1.0])
        np.testing.assert_array_equal(a.grad, expected_grad_a, "Subtraction backward pass failed for a.")
        np.testing.assert_array_equal(b.grad, expected_grad_b, "Subtraction backward pass failed for b.")

    def test_multiply_forward(self):
        a = Tensor(np.array([2.0, 3.0]))
        b = Tensor(np.array([4.0, 5.0]))
        c = multiply(a, b)
        expected = np.array([8.0, 15.0])
        np.testing.assert_array_equal(c.value, expected, "Multiplication forward pass failed.")

    def test_multiply_backward(self):
        a = Tensor(np.array([2.0, 3.0]))
        b = Tensor(np.array([4.0, 5.0]))
        c = multiply(a, b)
        c.backward(np.array([1.0, 1.0]))
        expected_grad_a = np.array([4.0, 5.0])
        expected_grad_b = np.array([2.0, 3.0])
        np.testing.assert_array_equal(a.grad, expected_grad_a, "Multiplication backward pass failed for a.")
        np.testing.assert_array_equal(b.grad, expected_grad_b, "Multiplication backward pass failed for b.")

    def test_dot_product_forward(self):
        a = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))
        b = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))
        c = dot_product(a, b)
        expected = np.dot(a.value, b.value)
        np.testing.assert_array_equal(c.value, expected, "Dot product forward pass failed.")

    def test_dot_product_backward(self):
        a = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))
        b = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))
        c = dot_product(a, b)
        c.backward(np.ones_like(c.value))
        expected_grad_a = np.dot(np.ones_like(c.value), b.value.T)
        expected_grad_b = np.dot(a.value.T, np.ones_like(c.value))
        np.testing.assert_array_equal(a.grad, expected_grad_a, "Dot product backward pass failed for a.")
        np.testing.assert_array_equal(b.grad, expected_grad_b, "Dot product backward pass failed for b.")

    def test_sui_sum_forward(self):
        a = Tensor(np.array([1.0, 2.0, 3.0]))
        c = sui_sum(a)
        expected = np.sum(a.value)
        self.assertEqual(c.value, expected, "Sum forward pass failed.")

    def test_sui_sum_backward(self):
        a = Tensor(np.array([1.0, 2.0, 3.0]))
        c = sui_sum(a)
        c.backward()
        expected_grad_a = np.array([1.0, 1.0, 1.0])
        np.testing.assert_array_equal(a.grad, expected_grad_a, "Sum backward pass failed for a.")

    def test_relu_forward(self):
        a = Tensor(np.array([-1.0, 0.0, 1.0]))
        c = relu(a)
        expected = np.maximum(0, a.value)
        np.testing.assert_array_equal(c.value, expected, "ReLU forward pass failed.")

    def test_relu_backward(self):
        a = Tensor(np.array([-1.0, 0.0, 1.0]))
        c = relu(a)
        c.backward(np.array([1.0, 1.0, 1.0]))
        expected_grad_a = np.array([0.0, 0.0, 1.0])
        np.testing.assert_array_equal(a.grad, expected_grad_a, "ReLU backward pass failed for a.")

    def test_chain_rule(self):
        # Testing a simple computation chain: z = sum(relu(a * b + c))
        a = Tensor(np.array([1.0, -2.0, 3.0]))
        b = Tensor(np.array([4.0, 5.0, -6.0]))
        c = Tensor(np.array([7.0, -8.0, 9.0]))
        d = add(multiply(a, b), c)
        e = relu(d)
        z = sui_sum(e)
        z.backward()

        # Manually compute expected gradients
        d_value = a.value * b.value + c.value
        relu_grad = (d_value > 0).astype(float)
        grad_e = np.ones_like(e.value)
        grad_d = grad_e * relu_grad
        grad_a = grad_d * b.value
        grad_b = grad_d * a.value
        grad_c = grad_d

        np.testing.assert_array_equal(a.grad, grad_a, "Chain rule backward pass failed for a.")
        np.testing.assert_array_equal(b.grad, grad_b, "Chain rule backward pass failed for b.")
        np.testing.assert_array_equal(c.grad, grad_c, "Chain rule backward pass failed for c.")

if __name__ == '__main__':
    unittest.main()
